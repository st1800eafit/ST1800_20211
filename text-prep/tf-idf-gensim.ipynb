{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cvTkPqJE7KF"
      },
      "source": [
        "# TF-IDF utilizando Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knaRLaYKE_tQ"
      },
      "source": [
        "## Importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "source": [
        "import gensim\n",
        "import pprint\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPlvwZ9UFEUi"
      },
      "source": [
        "## Lectura de los archivos\n",
        "\n",
        "En este punto, la idea es tener en un dataframe de pandas tanto el nombre del documento (nombre de archivo), como su contenido"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "7qkoNkAt1tia",
        "outputId": "825d9b59-510a-463c-b40d-ceb18c127600"
      },
      "source": [
        "doc_list = [\n",
        "   \"Hello, how are you?\", \"How do you do?\", \n",
        "   \"Hey what are you doing? yes you What are you doing?\"\n",
        "]\n",
        "\n",
        "doc_list\n",
        "df = pd.DataFrame({\"doc\": [1,2,3],\"content\": doc_list})\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Hello, how are you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>How do you do?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Hey what are you doing? yes you What are you d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doc                                            content\n",
              "0    1                                Hello, how are you?\n",
              "1    2                                     How do you do?\n",
              "2    3  Hey what are you doing? yes you What are you d..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JnmYYRTFSy6"
      },
      "source": [
        "## Preparación de textos\n",
        "\n",
        "Para comenzar cualquier tarea de NLP (clasificación, recomendación, etc.) se debe preparar cada documento del *corpus*.\n",
        "\n",
        "Normalmente esto incluye tareas como:\n",
        "\n",
        "- Tokenización\n",
        "- Quitar *stop words*\n",
        "- Quitar caracteres especiales.\n",
        "- Lematización y/o *Stemming* (¿Cúal es mejor?)\n",
        "- Creación de un diccionario\n",
        "- Creación de una bolsa de palabras (*BoW*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTv0jK-_F3KE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF9bPT2WF1s8"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4P0JLOC-mSN"
      },
      "source": [
        "df[\"tokens\"] = df.content.apply(lambda doc: simple_preprocess(doc))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRj9gwJAAms9"
      },
      "source": [
        "En este punto, se pueden incluir mas pasos del preprocesing utilizando el `df[\"..\"].apply(lambda x: ...)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6M-zK32HIBW"
      },
      "source": [
        "### Creación del diccionario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-olWYIRL-qND"
      },
      "source": [
        "\n",
        "dictionary = corpora.Dictionary(df[\"tokens\"])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXIESGHWCQHS"
      },
      "source": [
        "El objeto **dictionary** es un arreglo de palabras únicas de todo el corpus (todos los documentos)\n",
        "\n",
        "`['are', 'hello', 'how', 'you', 'do', 'doing', 'hey', 'what', 'yes']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv5234OMHK4O"
      },
      "source": [
        "### Creación del BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSClC5Y3_MHc",
        "outputId": "0e98e4a8-ad9a-4e0e-dd85-c46501d179b5"
      },
      "source": [
        "raw_corpus = [dictionary.doc2bow(doc) for doc in df[\"tokens\"]]\n",
        "raw_corpus"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
              " [(2, 1), (3, 1), (4, 2)],\n",
              " [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kouFZeCKDMC1"
      },
      "source": [
        "*raw_corpus* es el famoso **Bag of Words**. El BoW es una matriz, donde cada fila representa un documento, y cada columna tiene una tupla, la cual tiene el formato *(id,count)*. Este *id* es la identificación (o posición) de cada palabra del documento dentro del diccionario.\n",
        "\n",
        "Para entender un poco más lo que nos dice el **BoW**, podemos utilizar el **dictionary** para extraer el token a partir de un id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TMDNZCEEPFH",
        "outputId": "6cc64c75-adf6-498f-fa2f-13732b85cad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for doc in raw_corpus:\n",
        "   print([[dictionary[id], freq] for id, freq in doc])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['are', 1], ['hello', 1], ['how', 1], ['you', 1]]\n",
            "[['how', 1], ['you', 1], ['do', 2]]\n",
            "[['are', 2], ['you', 3], ['doing', 2], ['hey', 1], ['what', 2], ['yes', 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdQDcNAHPJV"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "**Term Frecuency-Inverted Document Frecuency** tiene dos partes: \n",
        "- *Term Frecuency*: número de veces que esta una palabra en cada documento\n",
        "- *Inverted Document Frecuency*: Que tan rara es una palabra dentro del *corpus*\n",
        "\n",
        "Esta metrica nos ayuda a identificar esas palabras son relevantes para un documento, porque se repite varias veces dentro de él, pero es escasa entre los documentos.\n",
        "\n",
        "**NOTA:** no confundir con palabras con los topicos de tecnícas como LSA o LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYS3RmhLEBNk"
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(raw_corpus)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voybG-54ISdq"
      },
      "source": [
        "La primera parte de la tarea de **TF-IDF** ya fue realizada cuando creamos el *BoW*, lo que hace `TfidfModel` es la parte de IDF, el cual requiere que el `corpus` este dividio en diferentes *documentos*. Si todos los documentos son unidos en un gran *string*, esta fase de IDF no tendría sentido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3efq9Y3XALSb",
        "outputId": "9a479c97-bca7-4ac8-9347-428d112c809e"
      },
      "source": [
        "import numpy as np\n",
        "for doc in tfidf[raw_corpus]:\n",
        "   print([[dictionary[id], np.round(freq,2)] for id, freq in doc])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['are', 0.33], ['hello', 0.89], ['how', 0.33]]\n",
            "[['how', 0.18], ['do', 0.98]]\n",
            "[['are', 0.23], ['doing', 0.62], ['hey', 0.31], ['what', 0.62], ['yes', 0.31]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}